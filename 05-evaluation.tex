\subsection{Experimental Setup}
\label{sec:setup}

\subsubsection{System used}

We employ a server equipped with two Intel Xeon Gold 6226R processors, each featuring $16$ cores running at a clock speed of $2.90$ GHz. Each core is equipped with a $1$ MB L1 cache, a $16$ MB L2 cache, and a $22$ MB shared L3 cache. The system is configured with $376$ GB RAM and set up with CentOS Stream 8.


\subsubsection{Configuration}

We use 32-bit integers for vertex ids and 32-bit float for edge weights but use 64-bit floats for computations and hashtable values. We utilize $64$ threads to match the number of cores available on the system (unless specified otherwise). For compilation, we use GCC 8.5 and OpenMP 4.5.


\subsubsection{Dataset}

The graphs used in our experiments are given in Table \ref{tab:dataset}. These are sourced from the SuiteSparse Matrix Collection \cite{suite19}. In the graphs, number of vertices vary from $3.07$ to $214$ million, and number of edges vary from $25.4$ million to $3.80$ billion. We ensure edges to be undirected and weighted with a default of $1$.

\input{src/tab-dataset}
\input{src/fig-leiden-compare}
\input{src/fig-gve-compare}




\subsection{Comparing Performance of GVE-Leiden}

We now compare the performance of GVE-Leiden with the original Leiden \cite{com-traag19}, igraph Leiden \cite{csardi2006igraph}, and NetworKit Leiden \cite{staudt2016networkit}. For the original Leiden, we use a C++ program to initialize a \texttt{ModularityVertexPartition} upon the loaded graph, and invoke \texttt{optimise\_partition()} to obtain the community membership of each vertex in the graph. For igraph Leiden, we use \texttt{igraph\_community\_leiden()} with a resolution of $1/2|E|$, a beta of $0.01$, and request the algorithm to run until convergence. For NetworKit Leiden, we write a Python script to call \texttt{ParallelLeiden().run()}, while limiting the number of passes to $10$. For each graph, we measure the runtime of each implementation five times, for averaging. We also record the modularity of communities obtained, as reported by each implementation, save the community membership vector to a file, and later count the number of disconnected components with Algorithm \ref{alg:disconnected}. In all cases, we use modularity as the quality function to optimize for.

Figure \ref{fig:leiden-compare--runtime} shows the runtimes of the original Leiden, igraph Leiden, NetworKit Leiden, and GVE-Leiden on each graph in the dataset. Figure \ref{fig:leiden-compare--speedup} shows the speedup of GVE-Leiden with respect to each implementation mentioned above. GVE-Leiden is on average $373\times$, $86\times$, and $7.2\times$ faster than the original Leiden, igraph Leiden, and NetworKit Leiden respectively. On the \textit{sk-2005} graph, GVE-Leiden finds communities in $10.8$ seconds, and thus achieve a processing rate of $352$ million edges/s. Figure \ref{fig:leiden-compare--modularity} shows the modularity of communities obtained with each implementation. GVE-Leiden on average obtains $0.1\%$ lower modularity than the original Leiden and igraph Leiden, and $26\%$ higher modularity than NetworKit Leiden (especially on road networks and protein k-mer graphs). Finally, Figure \ref{fig:leiden-compare--disconnected} shows the fraction of disconnected communities obtained with each implementation. Here, absence of bars indicates the absence of disconnected communities. Communities identified by GVE-Leiden on average have $88\times$, $145\times$, and $0.76\times$ disconnected communities than the original Leiden, igraph Leiden, and NetworKit Leiden respectively. While this compares unfavorably with the original Leiden and igraph Leiden, it may be simpler to split the disconnected communities obtained from GVE-Leiden as a post-processing step. We would like to address this issue some time in the future.

%% Yet again on Leiden algorithm
Q> Why does Leiden have some disconnected communities still?
Q> Why does Leiden not work with road networks?
I think the answer for both questions has to do with parallelism. There is a race between threads to pick a suitable community. If the size of each community is large, this generally does not badly affect modularity. But with small community bounds (in refinement phase of Leiden) the race can lead to bad community memberships. I will try to come up a few solutions to this. This is not an issue with sequential, so Traag et al. dont observe this issue in their original paper.
https://www.nature.com/articles/s41598-019-41695-z




Next, we compare the performance of GVE-Leiden with GVE-Louvain \cite{sahu2023gvelouvain}. As above, for each graphs in the dataset, we run both algorithms 5 times to minimize measurement noise, and report the average in Figures \ref{fig:gve-compare--runtime}, \ref{fig:gve-compare--speedup}, \ref{fig:gve-compare--modularity}, and \ref{fig:gve-compare--disconnected}. Figure \ref{fig:gve-compare--runtime} shows the runtimes of GVE-Louvain and GVE-Leiden on each graph in the dataset. Figure \ref{fig:gve-compare--speedup} shows the speedup of GVE-Leiden with respect to GVE-Louvain. GVE-Leiden is on average $24\%$ slower than GVE-Louvain. This increase in computation time is a trade-off for obtaining significantly fewer disconnected communities, as given below. Figure \ref{fig:gve-compare--modularity} shows the modularity of communities obtained with GVE-Louvain and GVE-Leiden. GVE-Leiden on average obtains $0.3\%$ higher modularity than GVE-Louvain. Finally, Figure \ref{fig:gve-compare--disconnected} shows the fraction of disconnected communities obtained with GVE-Louvain and GVE-Leiden. Communities identified by GVE-Leiden on average has an $11$-fold decrease in the number of disconnected communities than GVE-Louvain.

\input{src/fig-leiden-splits}
\input{src/fig-leiden-hardness}
\input{src/fig-leiden-ss}




\subsection{Analyzing Performance of GVE-Leiden}

The phase-wise and pass-wise split of GVE-Leiden is shown in Figures \ref{fig:leiden-splits--phase} and \ref{fig:leiden-splits--pass} respectively. Figure \ref{fig:leiden-splits--phase} indicates that GVE-Leiden spends most of the runtime in the local-moving and refinement phases on \textit{web graphs}, \textit{road networks}, and \textit{protein k-mer graphs}, while it devotes majority of the runtime in the aggregation phase on \textit{social networks}. The pass-wise split (Figure \ref{fig:leiden-splits--pass}) indicates that the first pass dominates runtime on high-degree graphs (\textit{web graphs} and \textit{social networks}), while subsequent passes prevail in execution time on low-degree graphs (\textit{road networks} and \textit{protein k-mer graphs}).

On average, $37\%$ of GVE-Leiden's runtime is spent in the local-moving phase, $27\%$ in the refinement phase, $24\%$ is spent in the aggregation phase, and $12\%$ is spent in other steps (initialization, renumbering communities, looking up dendrogram, and resetting communities) of the algorithm. Further, $73\%$ of the runtime is spent in the first pass of the algorithm, which is the most expensive pass due to the size of the original graph (later passes work on super-vertex graphs).

We also observe that graphs with lower average degree (\textit{road networks} and \textit{protein k-mer graphs}) and graphs with poor community structure (such as \verb|com-LiveJournal| and \verb|com-Orkut|) have a larger $\text{runtime}/|E|$ factor, as shown in Figure \ref{fig:leiden-hardness}.




\subsection{Strong Scaling of GVE-Leiden}

Finally, we measure the strong scaling performance of GVE-Leiden. To this end, we adjust the number of threads from $1$ to $64$ in multiples of $2$ for each input graph, and measure the overall time taken for finding communities with GVE-Leiden, as well as its phase splits (local-moving, refinement, aggregation, others), five times for averaging. The results are shown in Figure \ref{fig:leiden-ss}. With 32 threads, GVE-Leiden obtains an average speedup of $10.9\times$ compared to running with a single thread, i.e., its performance increases by $1.6\times$ for every doubling of threads. Scaling is limited due to the various sequential steps/phases in the algorithm. At 64 threads, GVE-Leiden is impacted by NUMA effects, and offers speedup of only $12.9\times$.
